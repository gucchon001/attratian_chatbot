# 仕様書作成支援ボット MVP - テスト設計書

| ドキュメント | バージョン | 作成日 | 参照仕様書 |
| :--- | :--- | :--- | :--- |
| **テスト設計書** | **v1.0** | 2024/12/25 | SPEC-PL-001, SPEC-DS-001, SPEC-QA-001 |

---

## 🎯 1. テスト戦略

### 1.1. テスト目的
ハイブリッドアーキテクチャの動作確認と品質保証を行い、仕様書準拠の実装を検証する。

### 1.2. テスト対象範囲

#### **✅ テスト対象**
- **固定検索パイプライン** (Step1-5)
- **Agent機能** (回答生成・フォールバック検索)
- **ハイブリッド制御フロー**
- **エラーハンドリング**
- **UI統合機能**

#### **❌ テスト対象外**
- 外部API (Atlassian REST API) の詳細動作
- LLM (Gemini API) の内部処理
- ネットワーク・インフラレベルの問題

### 1.3. テスト分類

| テストレベル | 目的 | 実行頻度 | 自動化レベル |
| :--- | :--- | :--- | :--- |
| **ユニットテスト** | 個別コンポーネント検証 | CI/CD毎回 | 100%自動化 |
| **統合テスト** | コンポーネント間連携検証 | PR毎回 | 95%自動化 |
| **E2Eテスト** | エンドツーエンド動作検証 | リリース前 | 80%自動化 |
| **手動探索テスト** | UX・例外ケース検証 | リリース前 | 手動実行 |

---

## 🧪 2. ユニットテスト設計

### 2.1. ResponseGenerationAgent テスト

#### **テストID: UT-AG-001**

| テストケース | テスト内容 | 期待結果 |
| :--- | :--- | :--- |
| UT-AG-001-01 | 正常初期化 | Agent正常作成 |
| UT-AG-001-02 | LangChain未インストール | ImportError発生 |
| UT-AG-001-03 | 正常回答生成 | 構造化回答生成 |
| UT-AG-001-04 | 空検索結果対応 | 適切なフォールバック |
| UT-AG-001-05 | エラーハンドリング | エラー応答生成 |
| UT-AG-001-06 | 検索結果フォーマット | 正しい構造化 |
| UT-AG-001-07 | LLM接続検証 | 接続状態確認 |

#### **重要テストケース詳細**

```python
def test_generate_response_normal():
    """正常な回答生成の品質検証"""
    # Given: 高品質検索結果
    search_results = [
        {
            "source": "Confluence",
            "title": "ログイン機能仕様書", 
            "content": "OAuth 2.0認証フロー詳細...",
            "relevance_score": 0.9
        }
    ]
    
    # When: 回答生成
    response = agent.generate_response(search_results, "ログイン認証について")
    
    # Then: 構造化回答確認
    assert "## 📋 質問への回答" in response
    assert "## 🔍 詳細情報" in response  
    assert "## 📚 関連情報・参考資料" in response
    assert "## 💡 推奨アクション" in response
```

### 2.2. FallbackSearchAgent テスト

#### **テストID: UT-AG-002**

| テストケース | テスト内容 | 期待結果 |
| :--- | :--- | :--- |
| UT-AG-002-01 | ReAct Agent初期化 | Agent正常作成 |
| UT-AG-002-02 | フォールバック検索実行 | 探索的検索成功 |
| UT-AG-002-03 | キーワード拡張機能 | 関連語生成 |
| UT-AG-002-04 | 柔軟なJira検索 | 複数JQL戦略実行 |
| UT-AG-002-05 | 柔軟なConfluence検索 | 複数CQL戦略実行 |
| UT-AG-002-06 | Agent実行エラー | 緊急フォールバック |

### 2.3. AgentSelector テスト

#### **テストID: UT-AG-003**

| テストケース | テスト内容 | 期待結果 |
| :--- | :--- | :--- |
| UT-AG-003-01 | 高品質結果判定 | direct_response_generation |
| UT-AG-003-02 | 中品質結果判定 | enhanced_response_generation |
| UT-AG-003-03 | 低品質結果判定 | fallback_then_response |
| UT-AG-003-04 | 判断要素分析 | 適切な要素抽出 |
| UT-AG-003-05 | 戦略選択統計 | 選択履歴記録 |

### 2.4. 固定パイプライン テスト

#### **テストID: UT-PP-001 ~ 004**

| コンポーネント | 主要テストケース |
| :--- | :--- |
| **KeywordExtractor** | 日本語キーワード抽出、意図分析、フォールバック |
| **DataSourceJudge** | Jira/Confluence判定、複合判定、デフォルト選択 |
| **CQLSearchEngine** | 3段階検索戦略、結果ランキング、エラー処理 |
| **QualityEvaluator** | 3軸評価、品質スコア算出、しきい値判定 |

---

## 🔗 3. 統合テスト設計

### 3.1. ハイブリッド制御フロー テスト

#### **テストID: IT-HY-001**

**テストシナリオ: 高品質結果での直接回答生成**

```
1. Given: ユーザー質問「ログイン機能の仕様について」
2. When: ハイブリッド検索実行
3. Then: 
   - Step1-4で高品質結果 (score ≥ 0.75)
   - 回答生成Agent直接実行
   - 構造化された包括的回答生成
   - 信頼度情報付加
```

#### **テストID: IT-HY-002** 

**テストシナリオ: 低品質結果でのフォールバック検索**

```
1. Given: ユーザー質問「あいまいなキーワード」
2. When: ハイブリッド検索実行  
3. Then:
   - Step1-4で低品質結果 (score < 0.75)
   - フォールバック検索Agent実行
   - 探索的検索による結果補強
   - 拡張検索説明付きの回答生成
```

### 3.2. エラーケース統合テスト

#### **テストID: IT-ER-001 ~ 003**

| エラーケース | テスト内容 | 期待動作 |
| :--- | :--- | :--- |
| **API接続エラー** | Atlassian API応答なし | 適切なエラー応答・代替案提示 |
| **LLM API制限** | Gemini API レート制限 | フォールバック応答・再試行案内 |
| **Agent実行失敗** | LangChain Agent例外 | 簡易応答モードで継続 |

---

## 🌐 4. E2Eテスト設計

### 4.1. ユーザーシナリオベーステスト

#### **シナリオ 1: 新人開発者の仕様確認**

```
🎬 シナリオ: 新人がログイン機能の実装方法を調べる

1. サイドバーでフィルター設定
   - Confluence: ON
   - Jira: OFF  
   - 日付範囲: 最近3ヶ月

2. 質問入力: "ログイン機能はどのように実装されていますか？"

3. 期待結果:
   ✅ 3秒以内に応答開始
   ✅ プロセス可視化表示
   ✅ Confluence仕様書からの詳細情報
   ✅ OAuth認証フロー説明
   ✅ 実装に必要なステップ明示
   ✅ 関連ドキュメントリンク
```

#### **シナリオ 2: バグ調査エンジニアの問題解決**

```
🎬 シナリオ: バグ調査で関連情報を収集

1. フィルター設定
   - Confluence: ON
   - Jira: ON
   - プロジェクト: CTJ

2. 質問入力: "ログイン認証エラーの既知の問題はありますか？"

3. 期待結果:
   ✅ Jira + Confluence横断検索
   ✅ 過去の類似バグ情報
   ✅ 修正履歴・対処法
   ✅ 根本原因分析
   ✅ 再発防止策
```

#### **シナリオ 3: プロダクトマネージャーの機能理解**

```  
🎬 シナリオ: 機能仕様の全体把握

1. フィルター設定: デフォルト（全データソース）

2. 質問入力: "ユーザー認証機能の全体像を教えて"

3. 期待結果:
   ✅ 複数データソース統合情報
   ✅ 機能全体の俯瞰説明
   ✅ ビジネス価値・ユーザー影響
   ✅ 技術詳細と要件の関連
   ✅ 今後の拡張予定
```

### 4.2. 異常系E2Eテスト

#### **E2E-ER-001: API完全停止時の動作**

```
1. Given: Atlassian API完全停止
2. When: 各種質問を投入
3. Then: 
   ✅ 適切なエラーメッセージ表示
   ✅ 代替手段の提案
   ✅ システム停止せず継続動作
```

#### **E2E-ER-002: ネットワーク不安定時の動作**

```
1. Given: 断続的ネットワーク障害
2. When: 検索実行中に接続断
3. Then:
   ✅ タイムアウト処理
   ✅ 部分結果での応答生成  
   ✅ 再試行案内
```

---

## 📊 5. パフォーマンステスト設計

### 5.1. レスポンス時間テスト

| 測定項目 | 目標値 | 測定条件 |
| :--- | :--- | :--- |
| **初期化時間** | < 5秒 | アプリ起動〜初回応答可能 |
| **検索応答時間** | < 3秒 | クエリ入力〜回答表示開始 |
| **Agent処理時間** | < 10秒 | 複雑クエリの完全処理 |
| **UI応答性** | < 1秒 | フィルター操作〜反映 |

### 5.2. 負荷テスト

#### **同時ユーザーテスト**
- **軽負荷**: 5ユーザー同時 
- **中負荷**: 10ユーザー同時
- **高負荷**: 20ユーザー同時

#### **継続稼働テスト**  
- **短時間**: 1時間連続
- **中時間**: 8時間連続  
- **長時間**: 24時間連続

---

## 🛠️ 6. テスト実行環境・ツール

### 6.1. テスト環境

| 環境 | 用途 | 構成 |
| :--- | :--- | :--- |
| **開発環境** | ユニット・統合テスト | ローカル + モック |
| **ステージング環境** | E2Eテスト | 本番同等 + テストデータ |
| **本番環境** | 受け入れテスト | 実環境 + 実データ |

### 6.2. テストツール

#### **自動テストツール**
- **pytest**: ユニット・統合テスト実行
- **mock**: 外部依存のモック化
- **streamlit-test**: UI自動テスト
- **requests-mock**: API応答モック

#### **手動テストツール**  
- **Browser DevTools**: パフォーマンス測定
- **Postman**: API動作確認
- **Test Case Management**: テスト結果管理

### 6.3. テストデータ

#### **モックデータ**
```python
# 高品質検索結果サンプル
HIGH_QUALITY_RESULTS = [
    {
        "source": "Confluence",
        "title": "ログイン機能設計書 v2.0", 
        "content": "OAuth 2.0 + PKCE認証フロー...",
        "relevance_score": 0.92,
        "url": "https://test.atlassian.net/wiki/123"
    }
]

# 低品質検索結果サンプル
LOW_QUALITY_RESULTS = [
    {
        "source": "Jira",
        "title": "その他のタスク",
        "content": "関連性の低い内容...", 
        "relevance_score": 0.31,
        "url": "https://test.atlassian.net/browse/OTHER-1"
    }
]
```

---

## 🎯 7. テスト成功基準

### 7.1. ユニットテスト

- **テストカバレッジ**: 85%以上
- **成功率**: 100%
- **実行時間**: 全テスト30秒以内

### 7.2. 統合テスト

- **主要フロー**: 100%成功
- **エラーハンドリング**: 95%以上適切な処理
- **データ整合性**: 100%維持

### 7.3. E2Eテスト

- **ユーザーシナリオ**: 95%以上で期待結果
- **パフォーマンス**: 全項目で目標値達成
- **UX品質**: 手動確認で問題なし

### 7.4. 総合評価

| 項目 | 目標 | 必須レベル |
| :--- | :--- | :--- |
| **機能性** | 仕様書準拠100% | リリース必須 |
| **信頼性** | 99%可用性 | リリース必須 |
| **パフォーマンス** | 目標値90%達成 | リリース推奨 |
| **使いやすさ** | ユーザビリティテスト合格 | リリース推奨 |

---

## 📋 8. テスト実行計画

### 8.1. 実行フェーズ

| フェーズ | 期間 | テスト内容 | 実行者 |
| :--- | :--- | :--- | :--- |
| **Phase 1** | 1日 | ユニットテスト実装・実行 | 開発者 |
| **Phase 2** | 1日 | 統合テスト実装・実行 | 開発者 |
| **Phase 3** | 0.5日 | E2Eテスト実行 | QA + 開発者 |
| **Phase 4** | 0.5日 | 手動探索テスト | QA + ユーザー |

### 8.2. 不具合対応

#### **重要度分類**
- **Critical**: システム停止・データ損失
- **High**: 主要機能不具合
- **Medium**: 部分的機能問題
- **Low**: UI・UX改善

#### **対応優先度**
1. **Critical**: 即座修正 (24時間以内)
2. **High**: 優先修正 (3日以内)
3. **Medium**: 計画修正 (1週間以内)
4. **Low**: 今後対応 (次リリース)

---

*最終更新: 2024年12月25日 - v1.0* 