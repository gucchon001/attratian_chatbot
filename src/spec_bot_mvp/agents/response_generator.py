"""
å›ç­”ç”ŸæˆAgent

ä»•æ§˜æ›¸å®šç¾© (SPEC-DS-001 4.1):
- å½¹å‰²: æ¤œç´¢çµæœã‚’çµ±åˆãƒ»è¦ç´„ã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ
- ã‚¿ã‚¤ãƒ—: LLMChainï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ç„¡ã—ï¼‰
- å…¥åŠ›: æ¤œç´¢çµæœ + ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
- å‡ºåŠ›: çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”
"""

import logging
from typing import Dict, List, Any
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from langchain.prompts import PromptTemplate
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_core.runnables import RunnableSequence
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

from ..config.settings import Settings

logger = logging.getLogger(__name__)

class ResponseGenerationAgent:
    """
    å›ç­”ç”ŸæˆAgent
    
    æ¤œç´¢çµæœã‚’çµ±åˆãƒ»è¦ç´„ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã›ãšã€LLMChainã«ã‚ˆã‚‹ç´”ç²‹ãªæ–‡ç« ç”Ÿæˆã«ç‰¹åŒ–ã€‚
    """
    
    def __init__(self):
        """AgentåˆæœŸåŒ–"""
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChainé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™: pip install langchain langchain-google-genai")
        
        self.settings = Settings()
        self._init_llm_chain()
        logger.info("âœ… ResponseGenerationAgentåˆæœŸåŒ–å®Œäº†")
    
    def _init_llm_chain(self):
        """LLMChainåˆæœŸåŒ–"""
        # Gemini LLMè¨­å®šï¼ˆsettings.iniæº–æ‹ ï¼‰
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.settings.gemini_model,  # settings.iniã‹ã‚‰èª­ã¿è¾¼ã¿
                api_key=self.settings.gemini_api_key,
                temperature=self.settings.gemini_temperature,  # settings.iniã‹ã‚‰èª­ã¿è¾¼ã¿
                max_output_tokens=self.settings.gemini_max_tokens  # settings.iniã‹ã‚‰èª­ã¿è¾¼ã¿
            )
            logger.info(f"âœ… Gemini LLMåˆæœŸåŒ–æˆåŠŸ: {self.settings.gemini_model}")
        except Exception as e:
            logger.error(f"âŒ Gemini LLMåˆæœŸåŒ–å¤±æ•—: {e}")
            raise
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.prompt = PromptTemplate(
            input_variables=["search_results", "user_query"],
            template=self._get_response_prompt_template()
        )
        
        # RunnableSequenceæ§‹ç¯‰ (æœ€æ–°LangChain API)
        self.chain = self.prompt | self.llm
    
    def _get_response_prompt_template(self) -> str:
        """CLIENTTOMOä»•æ§˜æ›¸ç‰¹åŒ– å›ç­”ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆv2.2 - é©å¿œçš„è©³ç´°ãƒ¬ãƒ™ãƒ«ç‰ˆï¼‰"""
        return """ã‚ãªãŸã¯CLIENTTOMOãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå°‚ç”¨ã®ä¸Šç´šä»•æ§˜æ›¸ä½œæˆæ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
é–‹ç™ºè€…ãƒ»PMãƒ»CSãƒãƒ¼ãƒ ãŒã€è¤‡é›‘ãªæ©Ÿèƒ½ä»•æ§˜ã‚’ç´ æ—©ãç†è§£ã—ã€å®Ÿè£…ã‚„ã‚µãƒãƒ¼ãƒˆã«æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€CLIENTTOMOãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©³ç´°æƒ…å ±ã€‘
â–  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦: ä¼æ¥­å‘ã‘ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
â–  å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: 
  - ä¼šå“¡ï¼ˆä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
  - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä¼æ¥­ï¼ˆæ³•äººãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰  
  - å…¨ä½“ç®¡ç†è€…ï¼ˆã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ï¼‰
â–  ä¸»è¦æ©Ÿèƒ½é ˜åŸŸ:
  - ãƒ­ã‚°ã‚¤ãƒ³ãƒ»èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¤šå±¤èªè¨¼ã€3ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ã‚¤ãƒ—å¯¾å¿œï¼‰
  - UI/UXï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã€ãƒ¢ãƒ¼ãƒ€ãƒ«ã€å‹•çš„æŒ™å‹•ï¼‰
  - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ã€æ¨©é™åˆ¶å¾¡ï¼‰
  - APIè¨­è¨ˆï¼ˆRESTfulã€èªè¨¼ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰
  - æ¥­å‹™ãƒ•ãƒ­ãƒ¼ï¼ˆPMãƒ»CSãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°é€£æºï¼‰
â–  æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯:
  - UI: Streamlitï¼ˆPythonï¼‰
  - AI: LangChain + Gemini 2.0-flash
  - æ¤œç´¢: 3æ®µéšCQLæ¤œç´¢æˆ¦ç•¥
  - ã‚¤ãƒ³ãƒ•ãƒ©: Confluence (CLIENTTOMOã‚¹ãƒšãƒ¼ã‚¹), Jira
  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥: SQLiteï¼ˆæ€§èƒ½å‘ä¸Šï¼‰

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã€‘
{user_query}

ã€æ¤œç´¢çµæœï¼ˆ80%é–¢é€£åº¦é”æˆã®3æ®µéšCQLæ¤œç´¢çµæœï¼‰ã€‘
{search_results}

ã€å›ç­”è©³ç´°ãƒ¬ãƒ™ãƒ«åˆ¤å®šã€‘
ä»¥ä¸‹ã®æ¡ä»¶ã«åŸºã¥ã„ã¦ã€é©åˆ‡ãªè©³ç´°ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š

â–  ç°¡æ½”ç‰ˆï¼ˆåŸºæœ¬å›ç­”ï¼‰ã‚’é¸æŠã™ã‚‹å ´åˆï¼š
- ä¸€èˆ¬çš„ãªæ©Ÿèƒ½èª¬æ˜è¦æ±‚ï¼ˆã€Œã€œã¨ã¯ï¼Ÿã€ã€Œã€œã«ã¤ã„ã¦æ•™ãˆã¦ã€ï¼‰
- æ¦‚è¦ç†è§£ç›®çš„ã®è³ªå•
- åˆå¿ƒè€…å‘ã‘ã®èª¬æ˜è¦æ±‚

â–  è©³ç´°ç‰ˆï¼ˆåŒ…æ‹¬çš„å›ç­”ï¼‰ã‚’é¸æŠã™ã‚‹å ´åˆï¼š
- å®Ÿè£…è©³ç´°è¦æ±‚ï¼ˆã€Œå®Ÿè£…æ–¹æ³•ã€ã€ŒæŠ€è¡“ä»•æ§˜ã€ã€ŒAPIè©³ç´°ã€ï¼‰
- è¤‡æ•°è¦³ç‚¹ã®çµ±åˆèª¬æ˜è¦æ±‚
- PMãƒ»CSå‘ã‘ã®æ¥­å‹™ãƒ•ãƒ­ãƒ¼è©³ç´°è¦æ±‚

ã€ç°¡æ½”ç‰ˆå›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## ğŸ¯ æ©Ÿèƒ½æ¦‚è¦
[è©²å½“æ©Ÿèƒ½ã®ç›®çš„ãƒ»å½¹å‰²ãƒ»ä¸»è¦ãªç‰¹å¾´ã‚’200æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«èª¬æ˜]

## ğŸ“‹ ä¸»è¦ä»•æ§˜
[å®Ÿè£…ãƒ»é‹ç”¨ã«å¿…è¦ãªé‡è¦ä»•æ§˜ã‚’è¦ç‚¹ã®ã¿ç®‡æ¡æ›¸ãã§è¨˜è¼‰]
- èªè¨¼æ–¹æ³•: [ç°¡æ½”ã«]
- å…¥åŠ›åˆ¶é™: [ä¸»è¦ãªã‚‚ã®ã®ã¿]
- ç”»é¢é·ç§»: [åŸºæœ¬ãƒ•ãƒ­ãƒ¼ã®ã¿]

## ğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ
[é–‹ç™ºãƒ»é‹ç”¨ã§ç‰¹ã«æ³¨æ„ã™ã¹ãè¦ç‚¹ã‚’3-4é …ç›®ã§]
- [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®æ³¨æ„ç‚¹]
- [å®Ÿè£…æ™‚ã®åˆ¶ç´„]
- [é‹ç”¨æ™‚ã®è€ƒæ…®äº‹é …]

---

ã€è©³ç´°ç‰ˆå›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## ğŸ¯ æ©Ÿèƒ½æ¦‚è¦
[è©²å½“æ©Ÿèƒ½ã®ç›®çš„ãƒ»å½¹å‰²ãƒ»CLIENTTOMOã«ãŠã‘ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾¡å€¤ã‚’èª¬æ˜]

## ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ä»•æ§˜
[ä¼šå“¡ãƒ»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä¼æ¥­ãƒ»ç®¡ç†è€…ãã‚Œãã‚Œã®åˆ©ç”¨æ–¹æ³•ãƒ»æ¨©é™ãƒ»ç”»é¢å·®ç•°]

## ğŸ”§ å®Ÿè£…ä»•æ§˜
[é–‹ç™ºè€…å‘ã‘æŠ€è¡“è©³ç´°ï¼šAPIä»•æ§˜ã€ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ç”»é¢é·ç§»ã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€é¸å®šæŠ€è¡“æ´»ç”¨æ–¹æ³•]

## ğŸ’¼ æ¥­å‹™ãƒ•ãƒ­ãƒ¼
[PMãƒ»CSå‘ã‘æ¥­å‹™è¦³ç‚¹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œæ‰‹é †ã€ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã€ã‚µãƒãƒ¼ãƒˆæ™‚ã®æ³¨æ„ç‚¹]

## ğŸ”— é–¢é€£æ©Ÿèƒ½ãƒ»ä¾å­˜é–¢ä¿‚
[ã“ã®æ©Ÿèƒ½ã«é–¢é€£ã™ã‚‹ä»–ã®CLIENTTOMOæ©Ÿèƒ½ãƒ»ç”»é¢ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®é€£æºãƒã‚¤ãƒ³ãƒˆ]

## âš ï¸ æ³¨æ„äº‹é …ãƒ»åˆ¶ç´„
[å®Ÿè£…æ™‚ã®åˆ¶ç´„ã€æ—¢çŸ¥ã®èª²é¡Œã€å°†æ¥ã®æ”¹å–„äºˆå®šã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶]

---

ã€é‡è¦æŒ‡é‡ã€‘
1. **é©å¿œæ€§**: è³ªå•ã®è¤‡é›‘ã•ã¨è©³ç´°è¦æ±‚ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é¸æŠ
2. **ç°¡æ½”æ€§**: ä¸è¦ãªæƒ…å ±ã¯å‰Šé™¤ã—ã€æ ¸å¿ƒã«é›†ä¸­
3. **å®Ÿç”¨æ€§**: ãƒãƒ¼ãƒ ãŒå³åº§ã«æ´»ç”¨ã§ãã‚‹æƒ…å ±ã«ç„¦ç‚¹
4. **å“è³ªç¶­æŒ**: ç°¡æ½”ç‰ˆã§ã‚‚æ­£ç¢ºæ€§ã¨å®Œå…¨æ€§ã¯ä¿æŒ

å›ç­”:"""

    def generate_response(self, search_results: List[Dict], user_query: str, memory_context: str = "") -> str:
        """
        æ¤œç´¢çµæœã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆï¼ˆå…¨æ–‡å–å¾—å¯¾å¿œï¼‰
        
        Args:
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
            memory_context: å‰å›ã®æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
            
        Returns:
            çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”
        """
        try:
            logger.info("ğŸ’¡ å›ç­”ç”Ÿæˆé–‹å§‹: ã‚¯ã‚¨ãƒª='%s', çµæœæ•°=%d, ãƒ¡ãƒ¢ãƒªãƒ¼=%s", user_query, len(search_results), bool(memory_context))
            
            # Step 1: æ¤œç´¢çµæœã®å…¨æ–‡å–å¾—ã§å¼·åŒ–
            enhanced_results = self._enhance_content_with_full_fetch(search_results)
            enhanced_count = sum(1 for result in enhanced_results if result.get('content_enhanced', False))
            
            if enhanced_count > 0:
                logger.info(f"âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¼·åŒ–å®Œäº†: {enhanced_count}/{len(search_results)}ä»¶ã§å…¨æ–‡å–å¾—æˆåŠŸ")
            else:
                logger.info("â„¹ï¸ æ—¢å­˜ã®excerptã‚’ä½¿ç”¨ï¼ˆå…¨æ–‡å–å¾—ã¯ä¸è¦ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            
            # Step 2: å¼·åŒ–ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’æ§‹é€ åŒ–æ–‡å­—åˆ—ã«å¤‰æ›
            formatted_results = self._format_search_results(enhanced_results)
            
            # Step 3: ãƒ¡ãƒ¢ãƒªãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è³ªå•ã‚’æ‹¡å¼µ
            enhanced_query = user_query
            if memory_context:
                enhanced_query = f"{user_query}\n\nã€å‰å›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘{memory_context}"
            
            # Step 4: RunnableSequenceã§å›ç­”ç”Ÿæˆ (æœ€æ–°LangChain API)
            result = self.chain.invoke({
                "search_results": formatted_results,
                "user_query": enhanced_query
            })
            
            # Step 5: AIMessageã‹ã‚‰æ–‡å­—åˆ—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            response = result.content if hasattr(result, 'content') else str(result)
            
            # Step 6: å‚ç…§å…ƒæƒ…å ±ã¨æ·±æ˜ã‚Šææ¡ˆã‚’è¿½åŠ 
            enhanced_response = self._enhance_response_with_sources(response, enhanced_results, user_query)
            
            # Step 7: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¼·åŒ–ã®çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
            confluence_enhanced = sum(1 for result in enhanced_results 
                                    if result.get('content_enhanced', False) and 
                                    (result.get('datasource') == 'confluence' or result.get('source') == 'confluence'))
            jira_enhanced = enhanced_count - confluence_enhanced
            
            if enhanced_count > 0:
                stats_parts = []
                if confluence_enhanced > 0:
                    stats_parts.append(f"Confluence: {confluence_enhanced}ä»¶ï¼ˆç„¡åˆ¶é™å…¨æ–‡å–å¾—ï¼‰")
                if jira_enhanced > 0:
                    stats_parts.append(f"Jira: {jira_enhanced}ä»¶ï¼ˆè©³ç´°å–å¾—ï¼‰")
                    
                stats_detail = " | ".join(stats_parts)
                stats_info = f"\n\n---\n**ğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—çŠ¶æ³**: {enhanced_count}/{len(search_results)}ä»¶ã§è©³ç´°ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—\n**ğŸ” å–å¾—è©³ç´°**: {stats_detail}\n**ğŸ’¡ åŠ¹æœ**: ã‚ˆã‚ŠåŒ…æ‹¬çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›"
                enhanced_response += stats_info
            else:
                stats_info = f"\n\n---\n**ğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—çŠ¶æ³**: æ—¢å­˜ã®è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆ{len(search_results)}ä»¶ï¼‰"
                enhanced_response += stats_info
            
            logger.info("âœ… å›ç­”ç”Ÿæˆå®Œäº†: æ–‡å­—æ•°=%d, å¼·åŒ–ä»¶æ•°=%d", len(enhanced_response), enhanced_count)
            return enhanced_response
            
        except Exception as e:
            logger.error("âŒ å›ç­”ç”Ÿæˆå¤±æ•—: %s", str(e))
            return self._generate_error_response(user_query, str(e))
    
    def _enhance_response_with_sources(self, response: str, search_results: List[Dict], user_query: str) -> str:
        """
        å›ç­”ã«å‚ç…§å…ƒæƒ…å ±ã¨æ·±æ˜ã‚Šææ¡ˆã‚’è¿½åŠ 
        
        Args:
            response: ç”Ÿæˆã•ã‚ŒãŸå›ç­”
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
            
        Returns:
            æ‹¡å¼µã•ã‚ŒãŸå›ç­”
        """
        # å‚ç…§å…ƒæƒ…å ±ã‚’ç”Ÿæˆ
        sources_section = self._generate_sources_section(search_results)
        
        # æ·±æ˜ã‚Šææ¡ˆã‚’ç”Ÿæˆ
        followup_section = self._generate_followup_suggestions(search_results, user_query)
        
        # å›ç­”ã«è¿½åŠ 
        if sources_section:
            response += f"\n\n{sources_section}"
        
        if followup_section:
            response += f"\n\n{followup_section}"
            
        return response
    
    def _generate_sources_section(self, search_results: List[Dict]) -> str:
        """
        å‚ç…§å…ƒæƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        
        Args:
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            
        Returns:
            å‚ç…§å…ƒæƒ…å ±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ–‡å­—åˆ—
        """
        if not search_results:
            return ""
        
        sources_lines = ["## ğŸ“š å‚è€ƒæ–‡çŒ®ãƒ»æƒ…å ±æº"]
        
        for i, result in enumerate(search_results, 1):
            title = result.get('title', f'æ–‡æ›¸ {i}')
            url = result.get('url', result.get('link', ''))
            source = result.get('source', 'Unknown')
            
            if url:
                sources_lines.append(f"ğŸ“„ **{title}**")
                sources_lines.append(f"ğŸ”— {url}")
                sources_lines.append("")  # ç©ºè¡Œ
            else:
                # URLãŒãªã„å ´åˆã¯ã‚½ãƒ¼ã‚¹æƒ…å ±ã®ã¿
                sources_lines.append(f"ğŸ“„ **{title}** ({source})")
                sources_lines.append("")
        
        return "\n".join(sources_lines)
    
    def _generate_followup_suggestions(self, search_results: List[Dict], user_query: str) -> str:
        """
        ã•ã‚‰ãªã‚‹æ·±æ˜ã‚Šææ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        
        Args:
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ  
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
            
        Returns:
            æ·±æ˜ã‚Šææ¡ˆã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ–‡å­—åˆ—
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        query_keywords = self._extract_query_keywords(user_query)
        
        # æ¤œç´¢çµæœã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        result_keywords = self._extract_result_keywords(search_results)
        
        # é–¢é€£ææ¡ˆã‚’ç”Ÿæˆ
        suggestions = []
        
        # åŸºæœ¬çš„ãªæ·±æ˜ã‚Šææ¡ˆï¼ˆå‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        primary_keyword = query_keywords[0] if query_keywords else None
        
        # æ±ç”¨çš„ãªææ¡ˆç”Ÿæˆï¼ˆå›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å»ƒæ­¢ï¼‰
        if primary_keyword:
            suggestions.extend([
                f"{primary_keyword}ã®æŠ€è¡“ä»•æ§˜ã‚’è©³ã—ãçŸ¥ã‚ŠãŸã„",
                f"{primary_keyword}ã®é‹ç”¨æ‰‹é †ã‚’ç¢ºèªã—ãŸã„",
                f"{primary_keyword}ã¨ã®é€£æºæ–¹æ³•ã‚’è¦‹ãŸã„"
            ])
        
        # æ¤œç´¢çµæœã‹ã‚‰å‹•çš„ã«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ææ¡ˆ
        if result_keywords and len(result_keywords) > 1:
            secondary_keyword = result_keywords[1] if result_keywords[1] != primary_keyword else result_keywords[0]
            suggestions.append(f"{secondary_keyword}ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®ææ¡ˆï¼ˆæ¤œç´¢çµæœã‹ã‚‰æ¨æ¸¬ï¼‰
        if any("ä¼šå“¡" in str(result) for result in search_results):
            suggestions.append("ä¼šå“¡å‘ã‘æ©Ÿèƒ½ã®è©³ç´°ä»•æ§˜ã‚’ç¢ºèªã—ãŸã„")
        if any("ä¼æ¥­" in str(result) for result in search_results):
            suggestions.append("ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä¼æ¥­å‘ã‘æ©Ÿèƒ½ã‚’ç¢ºèªã—ãŸã„")
        if any("ç®¡ç†è€…" in str(result) for result in search_results):
            suggestions.append("ç®¡ç†è€…å‘ã‘æ©Ÿèƒ½ã®è¨­å®šæ–¹æ³•ã‚’è¦‹ãŸã„")
        
        if not suggestions:
            return ""
        
        lines = ["## ğŸ¯ ã•ã‚‰ãªã‚‹æ·±æ˜ã‚Šãƒ»é–¢é€£æƒ…å ±"]
        for suggestion in suggestions[:3]:  # æœ€å¤§3ã¤ã«åˆ¶é™
            lines.append(f'- ã€Œ{suggestion}ã€')
        
        return "\n".join(lines)
    
    def _extract_query_keywords(self, user_query: str) -> List[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆå‹•çš„æŠ½å‡ºç‰ˆï¼‰"""
        keywords = []
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‹•çš„ã«æ¤œå‡º
        import re
        
        # å…·ä½“çš„ãªæŠ€è¡“ãƒ»æ©Ÿèƒ½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        technical_patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+æ©Ÿèƒ½',  # XXæ©Ÿèƒ½
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+è¨­è¨ˆ',  # XXè¨­è¨ˆ
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+ç®¡ç†',  # XXç®¡ç†
            r'[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]+èªè¨¼',  # XXèªè¨¼
            r'API[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]*',   # APIé–¢é€£
            r'UI[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]*',    # UIé–¢é€£
            r'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹[ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]*'  # DBé–¢é€£
        ]
        
        for pattern in technical_patterns:
            matches = re.findall(pattern, user_query)
            keywords.extend(matches)
        
        # åŸºæœ¬çš„ãªæ©Ÿèƒ½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        basic_keywords = ["èªè¨¼", "ç”»é¢", "ä»•æ§˜", "è¨­è¨ˆ", "å®Ÿè£…", "æ©Ÿèƒ½", "ã‚·ã‚¹ãƒ†ãƒ "]
        for keyword in basic_keywords:
            if keyword in user_query and keyword not in keywords:
                keywords.append(keyword)
        
        # é‡è¤‡é™¤å»ã¨æœ€å¤§5å€‹åˆ¶é™
        unique_keywords = list(dict.fromkeys(keywords))
        return unique_keywords[:5]
    
    def _extract_result_keywords(self, search_results: List[Dict]) -> List[str]:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        for result in search_results:
            title = result.get('title', '')
            content = result.get('content', '')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã‹ã‚‰ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            text = f"{title} {content}".lower()
            common_terms = ["èªè¨¼", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒ•ãƒ­ãƒ³ãƒˆ", "ãƒãƒƒã‚¯", "ãƒ†ã‚¹ãƒˆ"]
            
            for term in common_terms:
                if term in text:
                    keywords.add(term)
        
        return list(keywords)
    
    def _format_search_results(self, search_results: List[Dict]) -> str:
        """
        æ¤œç´¢çµæœã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå®Œå…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œï¼‰
        
        Args:
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸçµæœæ–‡å­—åˆ—
        """
        if not search_results:
            return "æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        formatted_sections = []
        
        for i, result in enumerate(search_results, 1):
            source = result.get('source', result.get('datasource', 'Unknown'))
            title = result.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å„ªå…ˆé †ä½: content > excerpt > summary
            content = result.get('content')
            if not content:
                content = result.get('excerpt', '')
            if not content:
                content = result.get('summary', '')
            
            url = result.get('url', '')
            relevance_score = result.get('relevance_score', result.get('final_score', 0))
            
            # ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’å–å¾—å¯èƒ½ãªå ´åˆã¯è¿½åŠ 
            created = result.get('created', '')
            result_type = result.get('type', '')
            space = result.get('space', '')
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
            if content and len(content) > 500:
                # é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é©åˆ‡ã«è¦ç´„
                content_preview = content[:500] + "..."
                full_content_available = True
            else:
                content_preview = content
                full_content_available = False
            
            section = f"""
=== æ¤œç´¢çµæœ {i} ===
ã‚½ãƒ¼ã‚¹: {source}
ã‚¿ã‚¤ãƒˆãƒ«: {title}
é–¢é€£åº¦: {relevance_score:.3f}"""
            
            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            if space:
                section += f"\nã‚¹ãƒšãƒ¼ã‚¹: {space}"
            if result_type:
                section += f"\nã‚¿ã‚¤ãƒ—: {result_type}"
            if created:
                section += f"\nä½œæˆæ—¥: {created}"
            
            section += f"\nURL: {url}"
            
            section += f"""
å†…å®¹:
{content_preview}"""
            
            if full_content_available:
                section += "\n\nâ€» ã•ã‚‰ã«è©³ç´°ãªæƒ…å ±ãŒåˆ©ç”¨å¯èƒ½ã§ã™"
            
            formatted_sections.append(section)
        
        # æ¤œç´¢çµæœã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
        summary_section = f"""
=== æ¤œç´¢çµæœã‚µãƒãƒªãƒ¼ ===
ç·ä»¶æ•°: {len(search_results)}ä»¶
å¹³å‡é–¢é€£åº¦: {sum(result.get('relevance_score', result.get('final_score', 0)) for result in search_results) / len(search_results):.3f}
ä¸»è¦ã‚½ãƒ¼ã‚¹: {', '.join(set(result.get('source', result.get('datasource', 'Unknown')) for result in search_results))}
"""
        
        formatted_sections.insert(0, summary_section)
        
        return "\n".join(formatted_sections)
    
    def _generate_error_response(self, user_query: str, error_message: str) -> str:
        """
        ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ç”Ÿæˆ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            
        Returns:
            ã‚¨ãƒ©ãƒ¼å¿œç­”æ–‡å­—åˆ—
        """
        return f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

## âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°
**è³ªå•**: {user_query}
**ã‚¨ãƒ©ãƒ¼**: {error_message}

## ğŸ”§ å¯¾å‡¦æ–¹æ³•
1. **å†è©¦è¡Œ**: ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„
2. **è³ªå•ã®è¦‹ç›´ã—**: ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã‚’è¨€ã„æ›ãˆã¦ã¿ã¦ãã ã•ã„
3. **ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼èª¿æ•´**: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å¤‰æ›´ã—ã¦å¯¾è±¡ç¯„å›²ã‚’åºƒã’ã¦ã¿ã¦ãã ã•ã„

## ğŸ’¬ ã‚µãƒãƒ¼ãƒˆ
å•é¡ŒãŒç¶™ç¶šã™ã‚‹å ´åˆã¯ã€ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚
ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦ã”é€£çµ¡ã„ãŸã ã‘ã‚‹ã¨ã€ã‚ˆã‚Šè¿…é€Ÿãªå¯¾å¿œãŒå¯èƒ½ã§ã™ã€‚"""

    def validate_llm_connection(self) -> bool:
        """
        LLMæ¥ç¶šã®æ¤œè¨¼
        
        Returns:
            æ¥ç¶šæˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        """
        try:
            test_response = self.llm.invoke("ãƒ†ã‚¹ãƒˆ")
            logger.info("âœ… LLMæ¥ç¶šæ¤œè¨¼æˆåŠŸ")
            return True
        except Exception as e:
            logger.error("âŒ LLMæ¥ç¶šæ¤œè¨¼å¤±æ•—: %s", str(e))
            return False 

    def _enhance_content_with_full_fetch(self, search_results: List[Dict]) -> List[Dict]:
        """
        æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å…¨æ–‡å–å¾—ã§å¼·åŒ–
        
        300æ–‡å­—åˆ¶é™ã‚’æ’¤å»ƒã—ã€Confluenceãƒ‡ãƒ¼ã‚¿ã‚’ç„¡åˆ¶é™ã«å–å¾—
        
        Args:
            search_results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            
        Returns:
            å…¨æ–‡å–å¾—ã§å¼·åŒ–ã•ã‚ŒãŸæ¤œç´¢çµæœãƒªã‚¹ãƒˆ
        """
        enhanced_results = []
        
        for result in search_results:
            enhanced_result = result.copy()
            
            # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¢ºèª
            current_content = result.get('content') or result.get('excerpt', '')
            
            # Confluenceãƒšãƒ¼ã‚¸ã®å ´åˆã¯å¸¸ã«å…¨æ–‡å–å¾—ã‚’è©¦è¡Œï¼ˆç„¡åˆ¶é™ï¼‰
            if result.get('datasource') == 'confluence' or result.get('source') == 'confluence':
                logger.info(f"ğŸ”„ Confluenceå…¨æ–‡å–å¾—é–‹å§‹: {result.get('title', 'Unknown')}")
                full_content = self._fetch_full_page_content(result)
                if full_content and len(full_content) > len(current_content):
                    enhanced_result['content'] = full_content
                    enhanced_result['content_enhanced'] = True
                    logger.info(f"âœ… Confluenceå…¨æ–‡å–å¾—æˆåŠŸ: {result.get('title', 'Unknown')} ({len(current_content)}â†’{len(full_content)}æ–‡å­—)")
                else:
                    enhanced_result['content_enhanced'] = False
                    logger.warning(f"âš ï¸ Confluenceå…¨æ–‡å–å¾—å¤±æ•—ã¾ãŸã¯ã‚µã‚¤ã‚ºä¸å¤‰: {result.get('title', 'Unknown')}")
            
            # Jiraã®å ´åˆã¯å¾“æ¥é€šã‚ŠçŸ­ã„å ´åˆã®ã¿å…¨æ–‡å–å¾—
            elif (result.get('datasource') == 'jira' or result.get('source') == 'jira') and len(current_content) < 500:
                logger.info(f"ğŸ”„ Jiraå…¨æ–‡å–å¾—é–‹å§‹: {result.get('title', 'Unknown')}")
                full_content = self._fetch_full_page_content(result)
                if full_content and len(full_content) > len(current_content):
                    enhanced_result['content'] = full_content
                    enhanced_result['content_enhanced'] = True
                    logger.info(f"âœ… Jiraå…¨æ–‡å–å¾—æˆåŠŸ: {result.get('title', 'Unknown')} ({len(current_content)}â†’{len(full_content)}æ–‡å­—)")
                else:
                    enhanced_result['content_enhanced'] = False
            else:
                enhanced_result['content_enhanced'] = False
            
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
    
    def _fetch_full_page_content(self, result: Dict) -> str:
        """
        å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®å…¨æ–‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        
        Args:
            result: æ¤œç´¢çµæœè¾æ›¸
            
        Returns:
            ãƒšãƒ¼ã‚¸å…¨æ–‡ (å–å¾—å¤±æ•—æ™‚ã¯ç©ºæ–‡å­—åˆ—)
        """
        try:
            # Confluenceãƒšãƒ¼ã‚¸ã®å ´åˆ
            if result.get('source') == 'confluence' or result.get('datasource') == 'confluence':
                return self._fetch_confluence_page_content(result)
            
            # Jiraã®å ´åˆ
            elif result.get('source') == 'jira' or result.get('datasource') == 'jira':
                return self._fetch_jira_issue_content(result)
            
            else:
                logger.warning(f"âš ï¸ ä¸æ˜ãªã‚½ãƒ¼ã‚¹å½¢å¼: {result.get('source', result.get('datasource', 'Unknown'))}")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ å…¨æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _fetch_confluence_page_content(self, result: Dict) -> str:
        """
        Confluenceãƒšãƒ¼ã‚¸ã®å…¨æ–‡å–å¾—
        
        Args:
            result: Confluenceæ¤œç´¢çµæœ
            
        Returns:
            ãƒšãƒ¼ã‚¸å…¨æ–‡
        """
        try:
            from atlassian import Confluence
            
            # APIæ¥ç¶šè¨­å®š
            confluence = Confluence(
                url=f"https://{self.settings.atlassian_domain}",
                username=self.settings.atlassian_email,
                password=self.settings.atlassian_api_token
            )
            
            # ãƒšãƒ¼ã‚¸IDã‚’å–å¾—
            page_id = result.get('id')
            if not page_id:
                logger.warning("âš ï¸ Confluenceãƒšãƒ¼ã‚¸IDä¸æ˜")
                return ""
            
            # ãƒšãƒ¼ã‚¸è©³ç´°ã‚’å–å¾—ï¼ˆbody.storageå½¢å¼ï¼‰
            page_content = confluence.get_page_by_id(
                page_id, 
                expand='body.storage,version,space'
            )
            
            if page_content and 'body' in page_content:
                storage_content = page_content['body']['storage']['value']
                
                # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡º
                import re
                clean_content = re.sub(r'<[^>]+>', '', storage_content)
                clean_content = re.sub(r'\s+', ' ', clean_content).strip()
                
                return clean_content
            else:
                logger.warning("âš ï¸ Confluenceãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—")
                return ""
                
        except ImportError:
            logger.warning("âš ï¸ atlassian-python-api not available")
            return ""
        except Exception as e:
            logger.error(f"âŒ Confluenceå…¨æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _fetch_jira_issue_content(self, result: Dict) -> str:
        """
        Jiraã‚¤ã‚·ãƒ¥ãƒ¼ã®å…¨æ–‡å–å¾—
        
        Args:
            result: Jiraæ¤œç´¢çµæœ
            
        Returns:
            ã‚¤ã‚·ãƒ¥ãƒ¼å…¨æ–‡
        """
        try:
            from atlassian import Jira
            
            # APIæ¥ç¶šè¨­å®š
            jira = Jira(
                url=f"https://{self.settings.atlassian_domain}",
                username=self.settings.atlassian_email,
                password=self.settings.atlassian_api_token
            )
            
            # ã‚¤ã‚·ãƒ¥ãƒ¼ã‚­ãƒ¼ã¾ãŸã¯IDã‚’å–å¾—
            issue_key = result.get('id') or result.get('key')
            if not issue_key:
                logger.warning("âš ï¸ Jiraã‚¤ã‚·ãƒ¥ãƒ¼ID/ã‚­ãƒ¼ä¸æ˜")
                return ""
            
            # ã‚¤ã‚·ãƒ¥ãƒ¼è©³ç´°ã‚’å–å¾—
            issue = jira.issue(issue_key, expand='renderedFields')
            
            if issue:
                # è¦ç´„ã€èª¬æ˜ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆ
                content_parts = []
                
                # è¦ç´„
                summary = issue.get('fields', {}).get('summary', '')
                if summary:
                    content_parts.append(f"è¦ç´„: {summary}")
                
                # èª¬æ˜
                description = issue.get('fields', {}).get('description')
                if description:
                    content_parts.append(f"èª¬æ˜: {description}")
                
                # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ¸ˆã¿èª¬æ˜ãŒã‚ã‚Œã°å„ªå…ˆ
                rendered_desc = issue.get('renderedFields', {}).get('description')
                if rendered_desc:
                    import re
                    clean_desc = re.sub(r'<[^>]+>', '', rendered_desc)
                    content_parts.append(f"è©³ç´°èª¬æ˜: {clean_desc}")
                
                return "\n\n".join(content_parts)
            else:
                logger.warning("âš ï¸ Jiraã‚¤ã‚·ãƒ¥ãƒ¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—")
                return ""
                
        except ImportError:
            logger.warning("âš ï¸ atlassian-python-api not available")
            return ""
        except Exception as e:
            logger.error(f"âŒ Jiraå…¨æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return "" 