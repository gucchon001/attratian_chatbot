"""
Confluence åŸºæœ¬æ¤œç´¢ãƒ„ãƒ¼ãƒ« - æ­£ã—ã„CQLæ¤œç´¢å®Ÿè£…

è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‹ã‚‰é©åˆ‡ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã€
æ®µéšçš„ãªCQLæ¤œç´¢æˆ¦ç•¥ã§é«˜ç²¾åº¦ãªçµæœã‚’è¿”ã—ã¾ã™ã€‚
"""

import logging
import re
from typing import List, Dict, Any, Optional
from atlassian import Confluence

from ..config.settings import settings
from ..utils.log_config import get_logger

logger = get_logger(__name__)


class ConfluenceBasicSearch:
    """
    åŸºæœ¬çš„ã§æ­£ç¢ºãªConfluenceæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    
    1. è‡ªç„¶è¨€èªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    2. æ®µéšçš„CQLæ¤œç´¢æˆ¦ç•¥
    3. ã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªçµæœå–å¾—
    """
    
    def __init__(self):
        """åŸºæœ¬æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.confluence = self._initialize_confluence()
        self.space_key = settings.confluence_space or "TEST"
        logger.info("ConfluenceBasicSearchåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_confluence(self) -> Confluence:
        """Confluenceæ¥ç¶šã®åˆæœŸåŒ–"""
        return Confluence(
            url=f"https://{settings.atlassian_domain}",
            username=settings.atlassian_email,
            password=settings.atlassian_api_token
        )
    
    def search(self, user_query: str) -> str:
        """
        åŸºæœ¬çš„ãªConfluenceæ¤œç´¢ã®å®Ÿè¡Œ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒª
            
        Returns:
            str: æ•´å½¢ã•ã‚ŒãŸæ¤œç´¢çµæœ
        """
        if not user_query or not user_query.strip():
            return "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        
        logger.info(f"åŸºæœ¬Confluenceæ¤œç´¢é–‹å§‹: '{user_query}'")
        
        try:
            # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = self._extract_keywords(user_query)
            if not keywords:
                return f"æ¤œç´¢å¯èƒ½ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{user_query}'"
            
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
            
            # 2. æ®µéšçš„æ¤œç´¢å®Ÿè¡Œ
            search_result = self._execute_progressive_search(keywords)
            
            if not search_result['results'] or search_result['results'].get('totalSize', 0) == 0:
                return f"ã€Œ{', '.join(keywords)}ã€ã«é–¢ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            # 3. ã‚¦ã‚§ã‚¤ãƒˆé©ç”¨ã«ã‚ˆã‚‹çµæœã®æ”¹è‰¯
            enhanced_results = self._apply_keyword_weighting(
                search_result['results']['results'],
                keywords
            )
            
            # çµæœã‚’å…ƒã®å½¢å¼ã«æˆ»ã™
            enhanced_search_result = search_result['results'].copy()
            enhanced_search_result['results'] = enhanced_results
            
            # 4. çµæœæ•´å½¢
            formatted_result = self._format_results(
                enhanced_search_result,
                keywords,
                user_query,
                search_result['strategy_used'] + " + Keyword Weighting",
                search_result['cql_query']
            )
            
            logger.info(f"åŸºæœ¬æ¤œç´¢å®Œäº†: {search_result['strategy_used']} ã§ {search_result['results'].get('totalSize', 0)}ä»¶")
            return formatted_result
            
        except Exception as e:
            logger.error(f"åŸºæœ¬Confluenceæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def _extract_keywords(self, user_query: str) -> List[str]:
        """
        è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‹ã‚‰CQLæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        """
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
        stop_words = {
            "ã«ã¤ã„ã¦", "æ•™ãˆã¦", "ãã ã•ã„", "ã®", "ã‚’", "ã«", "ã¯", "ãŒ", "ã§", "ã¨", "ã‹ã‚‰",
            "ã¾ã§", "ã‚ˆã‚Š", "ãªã©", "ã“ã¨", "ã‚‚ã®", "ãã‚Œ", "ã“ã‚Œ", "ã‚ã‚Œ", "ã©ã‚Œ", "ä½•",
            "ã©ã†", "ã©ã®", "ã„ã¤", "ã©ã“", "ãªãœ", "ã©ã†ã—ã¦", "ã™ã‚‹", "ã—ãŸ", "ã—ã¦",
            "ã„ã¾ã™", "ã¾ã™", "ã§ã™", "ã§ã‚ã‚‹", "ã ", "ãªã‚‹", "ãªã£ãŸ", "ãªã‚Š", "æ¢ã—ã¦", 
            "è¦‹ã¤ã‘ã¦", "ç¢ºèª", "èª¿ã¹ã¦", "çŸ¥ã‚ŠãŸã„"
        }
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        
        # 1. åŠ©è©ã§ã®åˆ†å‰²ã‚’è©¦è¡Œ
        for delimiter in ["ã®", "ã«ã¤ã„ã¦", "ã‚’", "ã«", "ã¯", "ãŒ"]:
            user_query = user_query.replace(delimiter, " ")
        
        # 2. æ–‡å­—åˆ—ã‚’å˜èªå€™è£œã«åˆ†å‰²ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ã‚«ã‚¿ã‚«ãƒŠã€ã²ã‚‰ãŒãªã€æ¼¢å­—ã€è‹±æ•°å­—ã®å¢ƒç•Œã§åˆ†å‰²
        words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]+|[ä¸€-é¾¯]+|[A-Za-z0-9]+', user_query)
        
        # 3. ã‚¹ãƒšãƒ¼ã‚¹ã§ã‚‚åˆ†å‰²
        space_words = user_query.split()
        words.extend(space_words)
        
        # 4. é‡è¦ãªå˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹åˆ¥å‡¦ç†
        important_patterns = [
            r'ãƒ­ã‚°ã‚¤ãƒ³[æ©Ÿèƒ½]*',
            r'API[è¨­è¨ˆ]*[æ›¸]*',
            r'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹[è¨­è¨ˆ]*',
            r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£[ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³]*',
            r'ãƒ†ã‚¹ãƒˆ[ä»•æ§˜æ›¸]*'
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, user_query)
            words.extend(matches)
        
        # 5. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã®æ•´ç†
        for word in words:
            word = word.strip()
            # é•·ã•2æ–‡å­—ä»¥ä¸Šã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã§ãªã„ã€ã²ã‚‰ãŒãªã®ã¿ã§ãªã„
            if (len(word) >= 2 and 
                word not in stop_words and 
                not re.match(r'^[ã-ã‚“]+$', word) and
                word not in ['æ©Ÿèƒ½ã®ä»•æ§˜ã«ã¤ã„ã¦æ•™ãˆã¦', 'ã‚’æ¢ã—ã¦ã„ã¾ã™', 'ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®è©³ç´°', 'ã«ã¤ã„ã¦']):
                keywords.append(word)
        
        # é‡è¤‡é™¤å»ã¨é †åºä¿æŒ
        unique_keywords = []
        for keyword in keywords:
            if keyword not in unique_keywords:
                unique_keywords.append(keyword)
        
        return unique_keywords[:5]  # æœ€å¤§5ã¤ã¾ã§
    
    def _execute_progressive_search(self, keywords: List[str]) -> Dict[str, Any]:
        """
        æ®µéšçš„æ¤œç´¢æˆ¦ç•¥ã®å®Ÿè¡Œ
        
        Args:
            keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict: æ¤œç´¢çµæœã¨ä½¿ç”¨ã•ã‚ŒãŸæˆ¦ç•¥
        """
        if not keywords:
            return {'results': None, 'strategy_used': 'No keywords', 'cql_query': None}
        
        main_keyword = keywords[0]
        
        # ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ä»¥å¤–ã«ã‚‚å¯¾å¿œï¼‰
        special_keywords = {
            "ãƒ­ã‚°ã‚¤ãƒ³": ["ãƒ­ã‚°ã‚¤ãƒ³", "èªè¨¼", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™»éŒ²", "æ¨©é™"],
            "æ€¥å‹Ÿ": ["æ€¥å‹Ÿ", "ç”³è¾¼", "ã‚ªãƒ—ã‚·ãƒ§ãƒ³", "å¥‘ç´„"],
            "API": ["API", "è¨­è¨ˆ", "ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"],
        }
        
        # ç‰¹åˆ¥å‡¦ç†ãŒå¿…è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡º
        special_keyword = None
        for keyword, related_terms in special_keywords.items():
            if keyword in main_keyword or any(keyword in kw for kw in keywords):
                special_keyword = keyword
                related_terms_list = related_terms
                break
        
        if special_keyword:
            logger.info(f"{special_keyword}æ©Ÿèƒ½æ¤œç´¢ã®ç‰¹åˆ¥å‡¦ç†ã‚’å®Ÿè¡Œ")
            
            # é–¢é€£èªã‚’ä½¿ã£ãŸåŒ…æ‹¬æ¤œç´¢ï¼ˆORæ¼”ç®—å­ä½¿ç”¨ï¼‰
            related_conditions = " OR ".join([f'title ~ "{term}"' for term in related_terms_list])
            text_conditions = " OR ".join([f'text ~ "{term}"' for term in related_terms_list])
            special_cql = f'space = "{self.space_key}" AND ({related_conditions} OR {text_conditions})'
            logger.info(f"{special_keyword}åŒ…æ‹¬æ¤œç´¢å®Ÿè¡Œ: {special_cql}")
            
            try:
                results = self.confluence.cql(special_cql, limit=20)
                if results and results.get('totalSize', 0) > 0:
                    return {
                        'results': results,
                        'strategy_used': f'{special_keyword} Comprehensive Search',
                        'cql_query': special_cql
                    }
            except Exception as e:
                logger.warning(f"{special_keyword}åŒ…æ‹¬æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æˆ¦ç•¥1: ã‚¿ã‚¤ãƒˆãƒ«å„ªå…ˆï¼ˆå…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        if len(keywords) >= 2:
            title_all_cql = self._build_title_search_cql(keywords)
            logger.info(f"æˆ¦ç•¥1å®Ÿè¡Œ: {title_all_cql}")
            
            try:
                results = self.confluence.cql(title_all_cql, limit=20)
                if results and results.get('totalSize', 0) > 0:
                    return {
                        'results': results,
                        'strategy_used': 'Strategy 1: Title All Keywords',
                        'cql_query': title_all_cql
                    }
            except Exception as e:
                logger.warning(f"æˆ¦ç•¥1ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æˆ¦ç•¥2: ã‚¿ã‚¤ãƒˆãƒ«ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        title_main_cql = f'space = "{self.space_key}" AND title ~ "{main_keyword}"'
        logger.info(f"æˆ¦ç•¥2å®Ÿè¡Œ: {title_main_cql}")
        
        try:
            results = self.confluence.cql(title_main_cql, limit=20)
            if results and results.get('totalSize', 0) > 0:
                return {
                    'results': results,
                    'strategy_used': 'Strategy 2: Title Main Keyword',
                    'cql_query': title_main_cql
                }
        except Exception as e:
            logger.warning(f"æˆ¦ç•¥2ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æˆ¦ç•¥3: ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼ˆå…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        if len(keywords) >= 2:
            text_all_cql = self._build_text_search_cql(keywords)
            logger.info(f"æˆ¦ç•¥3å®Ÿè¡Œ: {text_all_cql}")
            
            try:
                results = self.confluence.cql(text_all_cql, limit=20)
                if results and results.get('totalSize', 0) > 0:
                    return {
                        'results': results,
                        'strategy_used': 'Strategy 3: Text All Keywords',
                        'cql_query': text_all_cql
                    }
            except Exception as e:
                logger.warning(f"æˆ¦ç•¥3ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æˆ¦ç•¥4: ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰éƒ¨åˆ†æ¤œç´¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’éƒ¨åˆ†ã«åˆ†è§£ã—ã¦æ¤œç´¢ç¯„å›²ã‚’æ‹¡å¤§
        primary_parts = []
        if len(main_keyword) > 2:
            import re
            parts = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]+|[ä¸€-é¾¯]+|[A-Za-z0-9]+', main_keyword)
            primary_parts = [part for part in parts if len(part) >= 2]
        
        if primary_parts:
            # éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ORæ¤œç´¢ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ï¼‰
            part_conditions = " OR ".join([f'title ~ "{part}"' for part in primary_parts])
            fallback_cql = f'space = "{self.space_key}" AND ({part_conditions})'
            logger.info(f"æˆ¦ç•¥4å®Ÿè¡Œï¼ˆéƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰: {fallback_cql}")
        else:
            # å¾“æ¥ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_cql = f'space = "{self.space_key}" AND (title ~ "{main_keyword}" OR text ~ "{main_keyword}")'
            logger.info(f"æˆ¦ç•¥4å®Ÿè¡Œï¼ˆå¾“æ¥ï¼‰: {fallback_cql}")
        
        try:
            results = self.confluence.cql(fallback_cql, limit=20)
            return {
                'results': results,
                'strategy_used': 'Strategy 4: Enhanced Fallback with Parts',
                'cql_query': fallback_cql
            }
        except Exception as e:
            logger.error(f"æˆ¦ç•¥4ã‚¨ãƒ©ãƒ¼: {e}")
            return {'results': None, 'strategy_used': 'All strategies failed', 'cql_query': None}
    
    def _build_title_search_cql(self, keywords: List[str]) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ç”¨CQLã‚’æ§‹ç¯‰"""
        title_conditions = " AND ".join([f'title ~ "{keyword}"' for keyword in keywords])
        return f'space = "{self.space_key}" AND ({title_conditions})'
    
    def _build_text_search_cql(self, keywords: List[str]) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ç”¨CQLã‚’æ§‹ç¯‰"""
        text_conditions = " AND ".join([f'text ~ "{keyword}"' for keyword in keywords])
        return f'space = "{self.space_key}" AND ({text_conditions})'
    
    def _apply_keyword_weighting(self, results: List[Dict], keywords: List[str]) -> List[Dict]:
        """
        æ¤œç´¢çµæœã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¦ã‚§ã‚¤ãƒˆã‚’é©ç”¨ã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’æ”¹è‰¯
        
        Args:
            results: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            keywords: æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            
        Returns:
            List[Dict]: ã‚¦ã‚§ã‚¤ãƒˆé©ç”¨å¾Œã®çµæœï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰
        """
        if not results or not keywords:
            return results
        
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šï¼ˆæœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ï¼‰
        primary_keyword = keywords[0]
        
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†è§£
        primary_parts = []
        if len(primary_keyword) > 2:
            # ã€Œæ€¥å‹Ÿæ©Ÿèƒ½ã€â†’ã€Œæ€¥å‹Ÿã€ã€Œæ©Ÿèƒ½ã€ã«åˆ†è§£
            import re
            parts = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]+|[ä¸€-é¾¯]+|[A-Za-z0-9]+', primary_keyword)
            primary_parts = [part for part in parts if len(part) >= 2]
        
        weighted_results = []
        
        for result in results:
            title = self._safe_get_title(result)
            excerpt = self._safe_get_excerpt(result)
            
            # åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
            base_score = 0
            weighted_score = 0
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥ã‚¹ã‚³ã‚¢è¨ˆç®—
            for keyword in keywords:
                title_match = keyword in title
                content_match = keyword in excerpt
                
                if title_match:
                    base_score += 10
                if content_match:
                    base_score += 5
            
            # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰éƒ¨åˆ†ã®é«˜ã‚¦ã‚§ã‚¤ãƒˆé©ç”¨
            for part in primary_parts:
                title_match = part in title
                content_match = part in excerpt
                
                if title_match:
                    weighted_score += 50  # ä¸»è¦éƒ¨åˆ†ã‚¿ã‚¤ãƒˆãƒ«ä¸€è‡´ï¼ˆ5å€ã‚¦ã‚§ã‚¤ãƒˆï¼‰
                if content_match:
                    weighted_score += 25  # ä¸»è¦éƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´ï¼ˆ5å€ã‚¦ã‚§ã‚¤ãƒˆï¼‰
            
            # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
            if primary_keyword in title:
                weighted_score += 100  # å®Œå…¨ä¸€è‡´ã®è¶…é«˜ã‚¦ã‚§ã‚¤ãƒˆ
            
            total_score = base_score + weighted_score
            
            # çµæœã«ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’è¿½åŠ 
            enhanced_result = result.copy()
            enhanced_result['weighted_score'] = total_score
            enhanced_result['base_score'] = base_score
            enhanced_result['weight_bonus'] = weighted_score
            
            weighted_results.append(enhanced_result)
        
        # ã‚¦ã‚§ã‚¤ãƒˆé©ç”¨å¾Œã®ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        weighted_results.sort(key=lambda x: x.get('weighted_score', 0), reverse=True)
        
        logger.info(f"ã‚¦ã‚§ã‚¤ãƒˆé©ç”¨: {len(results)}ä»¶ â†’ ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{primary_keyword}' éƒ¨åˆ† {primary_parts}")
        
        return weighted_results

    def _format_results(self, results: Dict[str, Any], keywords: List[str], 
                       original_query: str, strategy_used: str, cql_query: str) -> str:
        """
        æ¤œç´¢çµæœã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«æ•´å½¢
        
        Args:
            results: Confluenceæ¤œç´¢çµæœ
            keywords: ä½¿ç”¨ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            original_query: å…ƒã®è³ªå•
            strategy_used: ä½¿ç”¨ã•ã‚ŒãŸæ¤œç´¢æˆ¦ç•¥
            cql_query: å®Ÿè¡Œã•ã‚ŒãŸCQLã‚¯ã‚¨ãƒª
            
        Returns:
            str: æ•´å½¢ã•ã‚ŒãŸæ¤œç´¢çµæœ
        """
        pages = results.get('results', [])
        total_count = results.get('totalSize', 0)
        
        if not pages:
            return f"ã€Œ{', '.join(keywords)}ã€ã«é–¢ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
        result_lines = [
            f"ğŸ“š **Confluenceæ¤œç´¢çµæœï¼ˆåŸºæœ¬æ¤œç´¢ï¼‰**",
            f"ğŸ” è³ªå•: ã€Œ{original_query}ã€",
            f"ğŸ”‘ æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}",
            f"âš™ï¸  æ¤œç´¢æˆ¦ç•¥: {strategy_used}",
            f"ğŸ“Š çµæœ: {len(pages)}ä»¶è¡¨ç¤ºï¼ˆç·æ•°: {total_count}ä»¶ï¼‰",
            "",
            f"ğŸ’» å®Ÿè¡ŒCQL: `{cql_query}`",
            ""
        ]
        
        # å„ãƒšãƒ¼ã‚¸ã®è©³ç´°
        for i, page in enumerate(pages[:5], 1):  # æœ€å¤§5ä»¶è¡¨ç¤º
            try:
                # ãƒšãƒ¼ã‚¸æƒ…å ±ã®å–å¾—
                title = self._safe_get_title(page)
                page_id = self._safe_get_page_id(page)
                space_info = self._safe_get_space_info(page)
                excerpt = self._safe_get_excerpt(page)
                
                # URLæ§‹ç¯‰
                page_url = f"https://{settings.atlassian_domain}/wiki/spaces/{space_info['key']}/pages/{page_id}"
                
                # ã‚¦ã‚§ã‚¤ãƒˆã‚¹ã‚³ã‚¢æƒ…å ±ã®å–å¾—
                weighted_score = page.get('weighted_score', 0)
                base_score = page.get('base_score', 0)
                weight_bonus = page.get('weight_bonus', 0)
                
                score_info = ""
                if weighted_score > 0:
                    score_info = f" (ã‚¹ã‚³ã‚¢: {weighted_score}"
                    if weight_bonus > 0:
                        score_info += f" = {base_score} + {weight_bonus}ãƒœãƒ¼ãƒŠã‚¹"
                    score_info += ")"
                
                result_lines.extend([
                    f"ğŸ“„ **{i}. {title}**{score_info}",
                    f"   ğŸ”— ãƒªãƒ³ã‚¯: {page_url}",
                ])
                
                if excerpt:
                    clean_excerpt = self._clean_html_tags(excerpt)[:200]
                    result_lines.append(f"   ğŸ“ æŠœç²‹: {clean_excerpt}...")
                
                result_lines.append("")
                
            except Exception as e:
                logger.warning(f"ãƒšãƒ¼ã‚¸ {i} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                result_lines.append(f"ğŸ“„ **{i}. [å‡¦ç†ã‚¨ãƒ©ãƒ¼]**")
                result_lines.append("")
        
        # æ®‹ã‚Šã®ä»¶æ•°è¡¨ç¤º
        if total_count > 5:
            result_lines.append(f"ğŸ“‹ ã•ã‚‰ã« {total_count - 5} ä»¶ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã™ã€‚")
            result_lines.append("")
        
        # åˆ©ç”¨ã®ãƒ’ãƒ³ãƒˆ
        result_lines.extend([
            "ğŸ’¡ **åˆ©ç”¨ã®ãƒ’ãƒ³ãƒˆ:**",
            "- ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã™ã‚‹ã¨ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™",
            "- ç‰¹å®šã®ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦è©³ã—ãèãã“ã¨ãŒã§ãã¾ã™",
            "- é–¢é€£ã™ã‚‹åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚‚æ¤œç´¢ã—ã¦ã¿ã¦ãã ã•ã„"
        ])
        
        return "\n".join(result_lines)
    
    def _safe_get_title(self, page: Dict[str, Any]) -> str:
        """å®‰å…¨ã«ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—"""
        return (page.get('title') or 
                page.get('content', {}).get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—'))
    
    def _safe_get_page_id(self, page: Dict[str, Any]) -> str:
        """å®‰å…¨ã«ãƒšãƒ¼ã‚¸IDã‚’å–å¾—"""
        return (page.get('id') or 
                page.get('content', {}).get('id', 'N/A'))
    
    def _safe_get_space_info(self, page: Dict[str, Any]) -> Dict[str, str]:
        """å®‰å…¨ã«ã‚¹ãƒšãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—"""
        space_info = (page.get('space') or 
                     page.get('content', {}).get('space', {}))
        
        if isinstance(space_info, dict):
            return {
                'name': space_info.get('name', self.space_key),
                'key': space_info.get('key', self.space_key)
            }
        else:
            return {'name': self.space_key, 'key': self.space_key}
    
    def _safe_get_excerpt(self, page: Dict[str, Any]) -> str:
        """å®‰å…¨ã«æŠœç²‹ã‚’å–å¾—"""
        return page.get('excerpt', '') or page.get('bodyExcerpt', '')
    
    def _clean_html_tags(self, text: str) -> str:
        """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹"""
        if not text:
            return ""
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        text = re.sub(r'<[^>]+>', '', text)
        
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        html_entities = {
            '&lt;': '<', '&gt;': '>', '&amp;': '&',
            '&quot;': '"', '&#39;': "'", '&nbsp;': ' '
        }
        
        for entity, char in html_entities.items():
            text = text.replace(entity, char)
        
        # ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã®é–¢æ•°
def search_confluence_basic(query: str) -> str:
    """
    åŸºæœ¬Confluenceæ¤œç´¢ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
        
    Returns:
        str: æ¤œç´¢çµæœ
    """
    searcher = ConfluenceBasicSearch()
    return searcher.search(query) 