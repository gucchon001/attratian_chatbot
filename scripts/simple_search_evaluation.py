"""
ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢ç²¾åº¦è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

åŸºæœ¬æ¤œç´¢ã€å¼·åŒ–æ¤œç´¢ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢ã®åŸºæœ¬æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
"""

import sys
import time
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from spec_bot.tools.confluence_tool import search_confluence_tool
from spec_bot.tools.confluence_enhanced_search import search_confluence_enhanced
from spec_bot.tools.confluence_indexer import ConfluenceIndexer
except ImportError as e:
    print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    sys.exit(1)

class SimpleSearchEvaluator:
    """ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.test_queries = [
            "APIè¨­è¨ˆä»•æ§˜æ›¸",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ", 
            "èªè¨¼æ©Ÿèƒ½",
            "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
            "ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †"
        ]
        
    def evaluate_result(self, result: str, query: str) -> dict:
        """çµæœã®ç°¡æ˜“è©•ä¾¡"""
        if not result or len(result) < 10:
            return {
                'quality_score': 0,
                'content_length': 0,
                'has_structure': False,
                'relevance': 0
            }
        
        content_length = len(result)
        has_structure = any(marker in result for marker in ['##', '**', 'ğŸ“„', 'ğŸ”', '-'])
        relevance = 1 if query.lower() in result.lower() else 0
        
        # å“è³ªã‚¹ã‚³ã‚¢ (0-10)
        quality = 0
        if content_length > 100: quality += 2
        if content_length > 500: quality += 2
        if content_length > 1000: quality += 2
        if has_structure: quality += 2
        if relevance: quality += 2
        
        return {
            'quality_score': quality,
            'content_length': content_length,
            'has_structure': has_structure,
            'relevance': relevance
        }
    
    def test_basic_search(self, query: str) -> dict:
        """åŸºæœ¬æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
        try:
            start_time = time.time()
            result = search_confluence_tool(query=query, analyze_content=True)
            execution_time = time.time() - start_time
            
            evaluation = self.evaluate_result(str(result), query)
            evaluation['method'] = 'åŸºæœ¬æ¤œç´¢'
            evaluation['execution_time'] = execution_time
            evaluation['success'] = True
            
            return evaluation
        except Exception as e:
            return {
                'method': 'åŸºæœ¬æ¤œç´¢',
                'execution_time': 0,
                'success': False,
                'error': str(e),
                'quality_score': 0,
                'content_length': 0,
                'has_structure': False,
                'relevance': 0
            }
    
    def test_enhanced_search(self, query: str) -> dict:
        """å¼·åŒ–æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
        try:
            start_time = time.time()
            result = search_confluence_enhanced(query=query, max_pages=5)
            execution_time = time.time() - start_time
            
            evaluation = self.evaluate_result(str(result), query)
            evaluation['method'] = 'å¼·åŒ–æ¤œç´¢'
            evaluation['execution_time'] = execution_time
            evaluation['success'] = True
            
            return evaluation
        except Exception as e:
            return {
                'method': 'å¼·åŒ–æ¤œç´¢',
                'execution_time': 0,
                'success': False,
                'error': str(e),
                'quality_score': 0,
                'content_length': 0,
                'has_structure': False,
                'relevance': 0
            }
    
    def test_indexed_search(self, query: str) -> dict:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
        try:
            indexer = ConfluenceIndexer()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
            if not indexer.load_index():
                return {
                    'method': 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢',
                    'execution_time': 0,
                    'success': False,
                    'error': 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                    'quality_score': 0,
                    'content_length': 0,
                    'has_structure': False,
                    'relevance': 0
                }
            
            start_time = time.time()
            results = indexer.search_by_keyword(query, max_results=5)
            execution_time = time.time() - start_time
            
            # çµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            if isinstance(results, list) and results:
                content = "\n".join([f"ğŸ“‹ {item.get('title', 'Unknown')}: {item.get('content_preview', '')[:100]}" 
                                   for item in results[:3]])
            else:
                content = "æ¤œç´¢çµæœãªã—"
            
            evaluation = self.evaluate_result(content, query)
            evaluation['method'] = 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢'
            evaluation['execution_time'] = execution_time
            evaluation['success'] = True
            evaluation['result_count'] = len(results) if isinstance(results, list) else 0
            
            return evaluation
        except Exception as e:
            return {
                'method': 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢',
                'execution_time': 0,
                'success': False,
                'error': str(e),
                'quality_score': 0,
                'content_length': 0,
                'has_structure': False,
                'relevance': 0
            }
    
    def run_evaluation(self):
        """è©•ä¾¡å®Ÿè¡Œ"""
        print("ğŸ” Confluenceæ¤œç´¢ç²¾åº¦ ç°¡æ˜“è©•ä¾¡")
        print("=" * 50)
        
        all_results = []
        
        for i, query in enumerate(self.test_queries, 1):
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ {i}/{len(self.test_queries)}: '{query}'")
            print("-" * 30)
            
            # å„æ¤œç´¢æ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆ
            basic_result = self.test_basic_search(query)
            enhanced_result = self.test_enhanced_search(query)
            indexed_result = self.test_indexed_search(query)
            
            results = [basic_result, enhanced_result, indexed_result]
            
            for result in results:
                if result['success']:
                    print(f"âœ… {result['method']}: "
                          f"{result['execution_time']:.2f}s, "
                          f"å“è³ª{result['quality_score']}/10, "
                          f"{result['content_length']}æ–‡å­—")
                else:
                    print(f"âŒ {result['method']}: {result.get('error', 'ã‚¨ãƒ©ãƒ¼')}")
                
                result['query'] = query
                all_results.append(result)
            
            # APIåˆ¶é™å¯¾ç­–
            time.sleep(1)
        
        self.generate_summary(all_results)
    
    def generate_summary(self, results):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\n{'='*50}")
        print("ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼")
        print(f"{'='*50}")
        
        methods = ['åŸºæœ¬æ¤œç´¢', 'å¼·åŒ–æ¤œç´¢', 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢']
        
        # çµ±è¨ˆè¨ˆç®—
        stats = {}
        for method in methods:
            method_results = [r for r in results if r['method'] == method and r['success']]
            
            if method_results:
                stats[method] = {
                    'success_count': len(method_results),
                    'total_tests': len([r for r in results if r['method'] == method]),
                    'avg_time': sum(r['execution_time'] for r in method_results) / len(method_results),
                    'avg_quality': sum(r['quality_score'] for r in method_results) / len(method_results),
                    'avg_length': sum(r['content_length'] for r in method_results) / len(method_results),
                    'structure_rate': sum(1 for r in method_results if r['has_structure']) / len(method_results) * 100
                }
            else:
                stats[method] = None
        
        # çµæœè¡¨ç¤º
        print(f"\nğŸ† ç·åˆæ¯”è¼ƒ")
        print(f"{'æ‰‹æ³•':<12} {'æˆåŠŸç‡':<8} {'å¹³å‡æ™‚é–“':<10} {'å¹³å‡å“è³ª':<10} {'æ§‹é€ åŒ–ç‡':<10}")
        print("-" * 55)
        
        for method in methods:
            if stats[method]:
                s = stats[method]
                success_rate = (s['success_count'] / s['total_tests']) * 100
                print(f"{method:<12} "
                      f"{success_rate:<8.1f}% "
                      f"{s['avg_time']:<10.2f}s "
                      f"{s['avg_quality']:<10.1f}/10 "
                      f"{s['structure_rate']:<10.1f}%")
            else:
                print(f"{method:<12} ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # æ¨å¥¨äº‹é …
        print(f"\nğŸ¯ æ¨å¥¨äº‹é …:")
        
        # æœ€å„ªç§€æ‰‹æ³•ã®ç‰¹å®š
        valid_methods = [method for method in methods if stats[method]]
        
        if valid_methods:
            best_quality = max(valid_methods, key=lambda m: stats[m]['avg_quality'])
            best_speed = max(valid_methods, key=lambda m: 1/stats[m]['avg_time'] if stats[m]['avg_time'] > 0 else 0)
            
            print(f"â€¢ æœ€é«˜å“è³ª: {best_quality} ({stats[best_quality]['avg_quality']:.1f}/10)")
            print(f"â€¢ æœ€é«˜é€Ÿåº¦: {best_speed} ({stats[best_speed]['avg_time']:.2f}s)")
            
            if stats.get('å¼·åŒ–æ¤œç´¢') and stats['å¼·åŒ–æ¤œç´¢']['avg_quality'] > 5:
                print("â€¢ æ¨å¥¨: å¼·åŒ–æ¤œç´¢ã‚’ä¸»è¦æ‰‹æ³•ã¨ã—ã¦æ¡ç”¨")
            if stats.get('ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢') and stats['ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢']['avg_time'] < 1.0:
                print("â€¢ æ¨å¥¨: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢ã‚’é«˜é€Ÿæ¤œç´¢ç”¨ã«ä½µç”¨")
        else:
            print("â€¢ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ æ¤œç´¢ç²¾åº¦è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")
    
    evaluator = SimpleSearchEvaluator()
    evaluator.run_evaluation()

if __name__ == "__main__":
    main() 